{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08e91cdb-4693-4632-b7aa-f37eec027131",
   "metadata": {},
   "source": [
    "## Working with Transformers in the HuggingFace Ecosystem\n",
    "\n",
    "In this laboratory exercise we will learn how to work with the HuggingFace ecosystem to adapt models to new tasks. As you will see, much of what is required is *investigation* into the inner-workings of the HuggingFace abstractions. With a little work, a little trial-and-error, it is fairly easy to get a working adaptation pipeline up and running."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e556105-269f-43e3-8933-227269afb9ea",
   "metadata": {},
   "source": [
    "### Exercise 1: Sentiment Analysis (warm up)\n",
    "\n",
    "In this first exercise we will start from a pre-trained BERT transformer and build up a model able to perform text sentiment analysis. Transformers are complex beasts, so we will build up our pipeline in several explorative and incremental steps.\n",
    "\n",
    "#### Exercise 1.1: Dataset Splits and Pre-trained model\n",
    "There are a many sentiment analysis datasets, but we will use one of the smallest ones available: the [Cornell Rotten Tomatoes movie review dataset](cornell-movie-review-data/rotten_tomatoes), which consists of 5,331 positive and 5,331 negative processed sentences from the Rotten Tomatoes movie reviews.\n",
    "\n",
    "**Your first task**: Load the dataset and figure out what splits are available and how to get them. Spend some time exploring the dataset to see how it is organized. Note that we will be using the [HuggingFace Datasets](https://huggingface.co/docs/datasets/en/index) library for downloading, accessing, splitting, and batching data for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6aa768b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset, get_dataset_split_names\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from transformers import DistilBertForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import AutoFeatureExtractor\n",
    "from transformers import DistilBertModel, DistilBertTokenizer, EarlyStoppingCallback\n",
    "from datasets import load_from_disk\n",
    "\n",
    "#import evaluate\n",
    "#import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15819af4-e850-412b-ab67-61b9b98e3a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 8530\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, get_dataset_split_names\n",
    "\n",
    "# data loading\n",
    "dataset_name= \"cornell-movie-review-data/rotten_tomatoes\"\n",
    "ds_train = load_dataset(dataset_name, split=\"train\")\n",
    "ds_test = load_dataset(dataset_name, split=\"test\")\n",
    "ds_validation = load_dataset(dataset_name, split=\"validation\")\n",
    "\n",
    "print(ds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0c3d455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text example: the way coppola professes his love for movies -- both colorful pop junk and the classics that unequivocally qualify as art -- is giddily entertaining .\n",
      "Label example: 1\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "idx = random.randint(0, ds_train.shape[0])\n",
    "print(f\"Text example: {ds_train['text'][idx]}\")\n",
    "print(f\"Label example: {ds_train['label'][idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27f50a0-0af4-4bdd-9a60-bab26f126c14",
   "metadata": {},
   "source": [
    "#### Exercise 1.2: A Pre-trained BERT and Tokenizer\n",
    "\n",
    "The model we will use is a *very* small BERT transformer called [Distilbert](https://huggingface.co/distilbert/distilbert-base-uncased) this model was trained (using self-supervised learning) on the same corpus as BERT but using the full BERT base model as a *teacher*.\n",
    "\n",
    "**Your next task**: Load the Distilbert model and corresponding tokenizer. Use the tokenizer on a few samples from the dataset and pass the tokens through the model to see what outputs are provided. I suggest you use the [`AutoModel`](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html) class (and the `from_pretrained()` method) to load the model and `AutoTokenizer` to load the tokenizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3eefc651-8551-44c2-8340-37d64660fc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f7df1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################## TOKENIZER ########################\n",
      "DistilBertTokenizerFast(name_or_path='distilbert/distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n",
      "\n",
      "######################## MODEL ########################\n",
      "DistilBertModel(\n",
      "  (embeddings): Embeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer): Transformer(\n",
      "    (layer): ModuleList(\n",
      "      (0-5): 6 x TransformerBlock(\n",
      "        (attention): DistilBertSdpaAttention(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (activation): GELUActivation()\n",
      "        )\n",
      "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print('######################## TOKENIZER ########################')\n",
    "print(tokenizer)\n",
    "print('\\n######################## MODEL ########################')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aedfa43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samlpe phrase: the film provides some great insight into the neurotic mindset of all comics -- even those who have reached the absolute top of the game .\n",
      "Token IDs: tensor([[  101,  1996,  2143,  3640,  2070,  2307, 12369,  2046,  1996, 11265,\n",
      "         10976,  4588,  9273,  3388,  1997,  2035,  5888,  1011,  1011,  2130,\n",
      "          2216,  2040,  2031,  2584,  1996,  7619,  2327,  1997,  1996,  2208,\n",
      "          1012,   102]]) \n",
      "\n",
      "BaseModelOutput(last_hidden_state=tensor([[[-0.1294, -0.1650, -0.0116,  ...,  0.0188,  0.5275,  0.3162],\n",
      "         [-0.1010, -0.4746,  0.0064,  ...,  0.2497,  1.0458, -0.1916],\n",
      "         [ 0.2940, -0.0451, -0.2083,  ...,  0.0923,  0.4090,  0.0165],\n",
      "         ...,\n",
      "         [ 0.2519, -0.5503,  0.3750,  ...,  0.0188,  0.3540,  0.4099],\n",
      "         [ 0.8860,  0.0582, -0.1454,  ...,  0.0152, -0.4178, -0.2869],\n",
      "         [ 0.7707,  0.2344,  0.0241,  ...,  0.0782, -0.2970, -0.0993]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "idx = 5\n",
    "input = tokenizer(ds_train[idx]['text'], padding=True, return_tensors=\"pt\")\n",
    "output = model(**input)\n",
    "\n",
    "print(\"Samlpe phrase:\", ds_train['text'][idx])\n",
    "print(\"Token IDs:\", input[\"input_ids\"], \"\\n\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7925df62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samlpe phrase: the story , like life , refuses to be simple , and the result is a compelling slice of awkward emotions .\n",
      "Token IDs: tensor([[  101,  1996,  2466,  1010,  2066,  2166,  1010, 10220,  2000,  2022,\n",
      "          3722,  1010,  1998,  1996,  2765,  2003,  1037, 17075, 14704,  1997,\n",
      "          9596,  6699,  1012,   102]]) \n",
      "\n",
      "BaseModelOutput(last_hidden_state=tensor([[[-0.1782, -0.1238, -0.0865,  ..., -0.0867,  0.4100,  0.3362],\n",
      "         [-0.2704, -0.0790, -0.4201,  ...,  0.1000,  0.8274, -0.1355],\n",
      "         [ 0.0728, -0.2583, -0.1176,  ..., -0.1747,  0.1437, -0.2213],\n",
      "         ...,\n",
      "         [-0.4658,  0.1920,  0.0862,  ..., -0.2875,  0.1009, -0.1235],\n",
      "         [ 0.7563,  0.0812, -0.2714,  ...,  0.3315, -0.5503, -0.4448],\n",
      "         [-0.5857, -0.1106,  0.3320,  ...,  0.0919,  0.3582, -0.0365]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "idx = 870\n",
    "input = tokenizer(ds_train[idx]['text'], padding=True, return_tensors=\"pt\")\n",
    "output = model(**input)\n",
    "\n",
    "print(\"Samlpe phrase:\", ds_train['text'][idx])\n",
    "print(\"Token IDs:\", input[\"input_ids\"], \"\\n\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0f6fd4-b80b-466d-b4ff-9aac0492c062",
   "metadata": {},
   "source": [
    "#### Exercise 1.3: A Stable Baseline\n",
    "\n",
    "In this exercise I want you to:\n",
    "1. Use Distilbert as a *feature extractor* to extract representations of the text strings from the dataset splits;\n",
    "2. Train a classifier (your choice, by an SVM from Scikit-learn is an easy choice).\n",
    "3. Evaluate performance on the validation and test splits.\n",
    "\n",
    "These results are our *stable baseline* -- the **starting** point on which we will (hopefully) improve in the next exercise.\n",
    "\n",
    "**Hint**: There are a number of ways to implement the feature extractor, but probably the best is to use a [feature extraction `pipeline`](https://huggingface.co/tasks/feature-extraction). You will need to interpret the output of the pipeline and extract only the `[CLS]` token from the *last* transformer layer. *How can you figure out which output that is?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2e57827",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cls_embeddings(texts, tokenizer, model, batch_size=16, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Converte una lista di testi in embeddings CLS usando DistilBERT.\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        encoded = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        encoded = {k: v.to(device) for k, v in encoded.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoded)\n",
    "            cls_embeddings = outputs.last_hidden_state[:,0,:]  # [CLS]\n",
    "        \n",
    "        all_embeddings.append(cls_embeddings.cpu().numpy())\n",
    "\n",
    "    return np.vstack(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ebb961ab-aa24-4d5c-86a1-c9b4237b40fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "\n",
    "train_texts, train_labels = ds_train[\"text\"], ds_train[\"label\"]\n",
    "val_texts, val_labels = ds_validation[\"text\"], ds_validation[\"label\"]\n",
    "test_texts, test_labels = ds_test[\"text\"], ds_test[\"label\"]\n",
    "\n",
    "# Estrazione embeddings CLS\n",
    "X_train = get_cls_embeddings(train_texts, tokenizer, model, device=DEVICE)\n",
    "X_val = get_cls_embeddings(val_texts, tokenizer, model, device=DEVICE)\n",
    "X_test = get_cls_embeddings(test_texts, tokenizer, model, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9e973cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.8217636022514071\n",
      "Test accuracy: 0.798311444652908\n",
      "\n",
      "Classification report (test):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.81      0.80       533\n",
      "           1       0.81      0.78      0.80       533\n",
      "\n",
      "    accuracy                           0.80      1066\n",
      "   macro avg       0.80      0.80      0.80      1066\n",
      "weighted avg       0.80      0.80      0.80      1066\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_train, train_labels)\n",
    "\n",
    "val_preds = clf.predict(X_val)\n",
    "test_preds = clf.predict(X_test)\n",
    "\n",
    "print(\"Validation accuracy:\", accuracy_score(val_labels, val_preds))\n",
    "print(\"Test accuracy:\", accuracy_score(test_labels, test_preds))\n",
    "print(\"\\nClassification report (test):\\n\", classification_report(test_labels, test_preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37141d1b-935b-425c-804c-b9b487853791",
   "metadata": {},
   "source": [
    "-----\n",
    "### Exercise 2: Fine-tuning Distilbert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a53f64-0238-42f2-bc20-86e51c77d2e5",
   "metadata": {},
   "source": [
    "In this exercise we will fine-tune the Distilbert model to (hopefully) improve sentiment analysis performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3392b1ed-597b-4a92-90fc-10eb11eac515",
   "metadata": {},
   "source": [
    "#### Exercise 2.1: Token Preprocessing\n",
    "\n",
    "The first thing we need to do is *tokenize* our dataset splits. Our current datasets return a dictionary with *strings*, but we want *input token ids* (i.e. the output of the tokenizer). This is easy enough to do my hand, but the HugginFace `Dataset` class provides convenient, efficient, and *lazy* methods. See the documentation for [`Dataset.map`](https://huggingface.co/docs/datasets/v3.5.0/en/package_reference/main_classes#datasets.Dataset.map).\n",
    "\n",
    "**Tip**: Verify that your new datasets are returning for every element: `text`, `label`, `intput_ids`, and `attention_mask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e6e2a95-5b08-4f81-b824-59a0fb3404e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1f833618a6640cd9f9bdc254b6154fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8530 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0d86e4a349c4be48652eb486ffd7363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "370da7b865bd4a64babc9959dae85955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(data):\n",
    "    return tokenizer(data[\"text\"], truncation=True)\n",
    "\n",
    "tokenized_ds_train = ds_train.map(tokenize_function, batched=True)\n",
    "tokenized_ds_validation = ds_validation.map(tokenize_function, batched=True)\n",
    "tokenized_ds_test = ds_test.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a39fe75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prima: dict_keys(['text', 'label'])\n",
      "Dopo: dict_keys(['text', 'label', 'input_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "print('Prima:', ds_train[idx].keys())\n",
    "print('Dopo:', tokenized_ds_train[idx].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fb2c82e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set elements: dict_keys(['text', 'label', 'input_ids', 'attention_mask'])\n",
      "Validation set elements: dict_keys(['text', 'label', 'input_ids', 'attention_mask'])\n",
      "Test set elements: dict_keys(['text', 'label', 'input_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "print('Train set elements:', tokenized_ds_train[idx].keys())\n",
    "print('Validation set elements:', tokenized_ds_validation[idx].keys())\n",
    "print('Test set elements:', tokenized_ds_test[idx].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a80de2-83c9-4c12-af4e-34babe23ffd1",
   "metadata": {},
   "source": [
    "#### Exercise 2.2: Setting up the Model to be Fine-tuned\n",
    "\n",
    "In this exercise we need to prepare the base Distilbert model for fine-tuning for a *sequence classification task*. This means, at the very least, appending a new, randomly-initialized classification head connected to the `[CLS]` token of the last transformer layer. Luckily, HuggingFace already provides an `AutoModel` for just this type of instantiation: [`AutoModelForSequenceClassification`](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#automodelforsequenceclassification). You will want you instantiate one of these for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a6327d73-3b71-478a-9932-bb062a650c57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "503d5d124aea4b57aa3e5fcbe9e67a80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d9ea7ed9a064059a103c2a02bbb476a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d109f7bd-955f-4fc4-bc3e-75bd99a17adf",
   "metadata": {},
   "source": [
    "#### Exercise 2.3: Fine-tuning Distilbert\n",
    "\n",
    "Finally. In this exercise you should use a HuggingFace [`Trainer`](https://huggingface.co/docs/transformers/main/en/trainer) to fine-tune your model on the Rotten Tomatoes training split. Setting up the trainer will involve (at least):\n",
    "\n",
    "\n",
    "1. Instantiating a [`DataCollatorWithPadding`](https://huggingface.co/docs/transformers/en/main_classes/data_collator) object which is what *actually* does your batch construction (by padding all sequences to the same length).\n",
    "2. Writing an *evaluation function* that will measure the classification accuracy. This function takes a single argument which is a tuple containing `(logits, labels)` which you should use to compute classification accuracy (and maybe other metrics like F1 score, precision, recall) and return a `dict` with these metrics.  \n",
    "3. Instantiating a [`TrainingArguments`](https://huggingface.co/docs/transformers/v4.51.1/en/main_classes/trainer#transformers.TrainingArguments) object using some reasonable defaults.\n",
    "4. Instantiating a `Trainer` object using your train and validation splits, you data collator, and function to compute performance metrics.\n",
    "5. Calling `trainer.train()`, waiting, waiting some more, and then calling `trainer.evaluate()` to see how it did.\n",
    "\n",
    "**Tip**: When prototyping this laboratory I discovered the HuggingFace [Evaluate library](https://huggingface.co/docs/evaluate/en/index) which provides evaluation metrics. However I found it to have insufferable layers of abstraction and getting actual metrics computed. I suggest just using the Scikit-learn metrics..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab17178d-5028-47e3-af97-953e8de5aae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer) # serve a creare bath dove le sequenze sono tutte della stessa lunghezza aggiungendo un padding dinamico che si adatta alla lunghezza della specifica sequenza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5d585d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, balanced_accuracy_score, precision_score, recall_score\n",
    "\n",
    "def compute_metrics(p):\n",
    "    logits, labels = p\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "    return {\n",
    "        \"balanced accuracy\": balanced_accuracy_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds, average=\"weighted\"),\n",
    "        \"precision\": precision_score(labels, preds, average=\"weighted\"),\n",
    "        \"recall\": recall_score(labels, preds, average=\"binary\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704cca4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4094850/503667280.py:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/home/gciacci/miniconda3/envs/transformers/lib/python3.13/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='68' max='3350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  68/3350 01:19 < 1:05:47, 0.83 it/s, Epoch 1/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Balanced accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.442600</td>\n",
       "      <td>0.338936</td>\n",
       "      <td>0.848968</td>\n",
       "      <td>0.848965</td>\n",
       "      <td>0.848999</td>\n",
       "      <td>0.844278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "early stopping required metric_for_best_model, but did not find eval_balanced_accuracy so early stopping is disabled\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"The `metric_for_best_model` training argument is set to 'eval_balanced_accuracy', which is not found in the evaluation metrics. The available evaluation metrics are: ['eval_loss', 'eval_balanced accuracy', 'eval_f1', 'eval_precision', 'eval_recall']. Consider changing the `metric_for_best_model` via the TrainingArguments.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/transformers/lib/python3.13/site-packages/transformers/trainer.py:3206\u001b[39m, in \u001b[36mTrainer._determine_best_metric\u001b[39m\u001b[34m(self, metrics, trial)\u001b[39m\n\u001b[32m   3205\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3206\u001b[39m     metric_value = \u001b[43mmetrics\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmetric_to_check\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   3207\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[31mKeyError\u001b[39m: 'eval_balanced_accuracy'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m      1\u001b[39m training_args = TrainingArguments(\n\u001b[32m      2\u001b[39m     output_dir=\u001b[33m\"\u001b[39m\u001b[33m./results\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      3\u001b[39m     eval_strategy=\u001b[33m\"\u001b[39m\u001b[33mepoch\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m \n\u001b[32m     14\u001b[39m )\n\u001b[32m     16\u001b[39m trainer = Trainer(\n\u001b[32m     17\u001b[39m     model=model,\n\u001b[32m     18\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     24\u001b[39m     callbacks=[EarlyStoppingCallback(early_stopping_patience=\u001b[32m3\u001b[39m)]\n\u001b[32m     25\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m trainer.evaluate()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/transformers/lib/python3.13/site-packages/transformers/trainer.py:2238\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2236\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2237\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2238\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2239\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/transformers/lib/python3.13/site-packages/transformers/trainer.py:2698\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2695\u001b[39m     \u001b[38;5;28mself\u001b[39m.control.should_training_stop = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2697\u001b[39m \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_epoch_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m-> \u001b[39m\u001b[32m2698\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2699\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rate\u001b[49m\n\u001b[32m   2700\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2702\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m DebugOption.TPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.debug:\n\u001b[32m   2703\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[32m   2704\u001b[39m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/transformers/lib/python3.13/site-packages/transformers/trainer.py:3138\u001b[39m, in \u001b[36mTrainer._maybe_log_save_evaluate\u001b[39m\u001b[34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[39m\n\u001b[32m   3136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.control.should_evaluate:\n\u001b[32m   3137\u001b[39m     metrics = \u001b[38;5;28mself\u001b[39m._evaluate(trial, ignore_keys_for_eval)\n\u001b[32m-> \u001b[39m\u001b[32m3138\u001b[39m     is_new_best_metric = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_determine_best_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3140\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.save_strategy == SaveStrategy.BEST:\n\u001b[32m   3141\u001b[39m         \u001b[38;5;28mself\u001b[39m.control.should_save = is_new_best_metric\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/transformers/lib/python3.13/site-packages/transformers/trainer.py:3208\u001b[39m, in \u001b[36mTrainer._determine_best_metric\u001b[39m\u001b[34m(self, metrics, trial)\u001b[39m\n\u001b[32m   3206\u001b[39m     metric_value = metrics[metric_to_check]\n\u001b[32m   3207\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m-> \u001b[39m\u001b[32m3208\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[32m   3209\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe `metric_for_best_model` training argument is set to \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_to_check\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, which is not found in the evaluation metrics. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3210\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe available evaluation metrics are: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(metrics.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Consider changing the `metric_for_best_model` via the TrainingArguments.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3211\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m   3213\u001b[39m operator = np.greater \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.greater_is_better \u001b[38;5;28;01melse\u001b[39;00m np.less\n\u001b[32m   3215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.best_metric \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyError\u001b[39m: \"The `metric_for_best_model` training argument is set to 'eval_balanced_accuracy', which is not found in the evaluation metrics. The available evaluation metrics are: ['eval_loss', 'eval_balanced accuracy', 'eval_f1', 'eval_precision', 'eval_recall']. Consider changing the `metric_for_best_model` via the TrainingArguments.\""
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=50,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds_train,\n",
    "    eval_dataset=tokenized_ds_validation,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8376de-8554-4a13-aac3-59257f3eb3fd",
   "metadata": {},
   "source": [
    "-----\n",
    "### Exercise 3: Choose at Least One\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b55cf4d-e64b-47fc-b8d5-37288b72d90d",
   "metadata": {},
   "source": [
    "#### Exercise 3.1: Efficient Fine-tuning for Sentiment Analysis (easy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f183856-1111-4fe9-81f1-691fe7c1b706",
   "metadata": {},
   "source": [
    "In Exercise 2 we fine-tuned the *entire* Distilbert model on Rotten Tomatoes. This is expensive, even for a small model. Find an *efficient* way to fine-tune Distilbert on the Rotten Tomatoes dataset (or some other dataset).\n",
    "\n",
    "**Hint**: You could check out the [HuggingFace PEFT library](https://huggingface.co/docs/peft/en/index) for some state-of-the-art approaches that should \"just work\". How else might you go about making fine-tuning more efficient without having to change your training pipeline from above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ea6bca5-9b36-424e-898c-52c0777eae5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eeca737-ee00-4d98-a3ae-f4d6eb3d264f",
   "metadata": {},
   "source": [
    "#### Exercise 3.2: Fine-tuning a CLIP Model (harder)\n",
    "\n",
    "Use a (small) CLIP model like [`openai/clip-vit-base-patch16`](https://huggingface.co/openai/clip-vit-base-patch16) and evaluate its zero-shot performance on a small image classification dataset like ImageNette or TinyImageNet. Fine-tune (using a parameter-efficient method!) the CLIP model to see how much improvement you can squeeze out of it.\n",
    "\n",
    "**Note**: There are several ways to adapt the CLIP model; you could fine-tune the image encoder, the text encoder, or both. Or, you could experiment with prompt learning.\n",
    "\n",
    "**Tip**: CLIP probably already works very well on ImageNet and ImageNet-like images. For extra fun, look for an image classification dataset with different image types (e.g. *sketches*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00b59ec2-4fe6-44c6-ab0b-0069f486bbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe42ed48-444d-47d4-bd8b-839a99e7996a",
   "metadata": {},
   "source": [
    "#### Exercise 3.3: Choose your Own Adventure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196f9129-ef2e-45f7-9e8f-baa697ccd91e",
   "metadata": {},
   "source": [
    "There are a *ton* of interesting and fun models on the HuggingFace hub. Pick one that does something interesting and adapt it in some way to a new task. Or, combine two or more models into something more interesting or fun. The sky's the limit.\n",
    "\n",
    "**Note**: Reach out to me by email or on the Discord if you are unsure about anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c150bd36-6535-4724-a06d-a61632d3132e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
